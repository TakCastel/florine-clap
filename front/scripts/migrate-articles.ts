/**
 * Script de migration des articles depuis florineclap.com vers Directus
 * 
 * Ce script :
 * 1. Scrape le site florineclap.com pour r√©cup√©rer tous les articles
 * 2. Extrait titre, sous-titre, contenu markdown, images
 * 3. T√©l√©charge les images et les upload dans Directus
 * 4. Cr√©e les articles dans Directus avec l'ordre exact
 * 
 * Usage: npx tsx scripts/migrate-articles.ts
 */

import axios from 'axios'
import * as cheerio from 'cheerio'
import TurndownService from 'turndown'
import { createDirectus, rest, staticToken, readItems, createItem, updateItem, deleteItems } from '@directus/sdk'
import type { Schema } from '../lib/directus-types'
import * as fs from 'fs'
import * as path from 'path'
import * as https from 'https'
import * as http from 'http'
import FormData from 'form-data'
import puppeteer from 'puppeteer'
import OpenAI from 'openai'
import * as dotenv from 'dotenv'

// Charger les variables d'environnement
dotenv.config({ path: path.join(process.cwd(), '.env') })

// Configuration
const SOURCE_URL = 'https://www.florineclap.com'
const BASE_PAGE_URL = `${SOURCE_URL}/blank/page`
const FIRST_PAGE = 1 // Commencer √† la page 1
const LAST_PAGE = 14
const ARTICLES_PER_PAGE = 5
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || ''

const DIRECTUS_URL = process.env.DIRECTUS_INTERNAL_URL || process.env.NEXT_PUBLIC_DIRECTUS_URL || 'http://localhost:8055'
const DIRECTUS_STATIC_TOKEN = process.env.DIRECTUS_STATIC_TOKEN || ''
const DIRECTUS_ADMIN_EMAIL = process.env.DIRECTUS_ADMIN_EMAIL || 'admin@example.com'
const DIRECTUS_ADMIN_PASSWORD = process.env.DIRECTUS_ADMIN_PASSWORD || 'admin'

// Mode dry-run : ne cr√©e pas les articles, juste affiche ce qui serait fait
const DRY_RUN = process.env.DRY_RUN === 'true' || process.argv.includes('--dry-run')

// Mode force : supprime tous les articles existants avant de migrer
const FORCE = process.env.FORCE === 'true' || process.argv.includes('--force')

// Mode update : met √† jour les articles existants au lieu de cr√©er des doublons
const UPDATE_EXISTING = process.env.UPDATE_EXISTING === 'true' || process.argv.includes('--update')

// Dossier temporaire pour les images
const TEMP_DIR = path.join(process.cwd(), '.temp-images')
if (!fs.existsSync(TEMP_DIR)) {
  fs.mkdirSync(TEMP_DIR, { recursive: true })
}

interface ScrapedArticle {
  title: string
  subtitle?: string
  content: string // Markdown
  images: string[] // URLs des images
  date?: string
  slug?: string
  order: number
  imageMap?: Map<string, string> // Map des URLs d'images vers les IDs Directus
}

// Initialiser Turndown pour convertir HTML en Markdown
const turndownService = new TurndownService({
  headingStyle: 'atx',
  codeBlockStyle: 'fenced',
  bulletListMarker: '-',
})

// Configuration personnalis√©e pour Turndown
turndownService.addRule('preserveImages', {
  filter: 'img',
  replacement: (content, node) => {
    const img = node as HTMLImageElement
    const src = img.getAttribute('src') || img.getAttribute('data-src') || ''
    const alt = img.getAttribute('alt') || ''
    const title = img.getAttribute('title') || ''
    return `![${alt}](${src}${title ? ` "${title}"` : ''})`
  }
})

/**
 * T√©l√©charge un fichier depuis une URL
 */
async function downloadFile(url: string, filepath: string): Promise<void> {
  return new Promise((resolve, reject) => {
    const protocol = url.startsWith('https') ? https : http
    const file = fs.createWriteStream(filepath)
    
    protocol.get(url, (response) => {
      if (response.statusCode === 301 || response.statusCode === 302) {
        // Suivre les redirections
        return downloadFile(response.headers.location!, filepath).then(resolve).catch(reject)
      }
      
      if (response.statusCode !== 200) {
        file.close()
        fs.unlinkSync(filepath)
        reject(new Error(`Failed to download: ${response.statusCode}`))
        return
      }
      
      response.pipe(file)
      file.on('finish', () => {
        file.close()
        resolve()
      })
    }).on('error', (err) => {
      file.close()
      if (fs.existsSync(filepath)) {
        fs.unlinkSync(filepath)
      }
      reject(err)
    })
  })
}

/**
 * Upload une image dans Directus
 */
async function uploadImageToDirectus(
  imagePath: string,
  filename: string,
  token: string
): Promise<string | null> {
  try {
    const formData = new FormData()
    formData.append('file', fs.createReadStream(imagePath), {
      filename: filename,
      contentType: 'image/jpeg', // Adapter selon le type d'image
    })

    const response = await axios.post(`${DIRECTUS_URL}/files`, formData, {
      headers: {
        ...formData.getHeaders(),
        'Authorization': `Bearer ${token}`,
      },
      maxContentLength: Infinity,
      maxBodyLength: Infinity,
    })

    return response.data.data.id
  } catch (error: any) {
    console.error(`Erreur upload image ${filename}:`, error.response?.data || error.message)
    return null
  }
}

/**
 * Obtient un token admin
 */
async function getAdminToken(): Promise<string> {
  const response = await fetch(`${DIRECTUS_URL}/auth/login`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ 
      email: DIRECTUS_ADMIN_EMAIL, 
      password: DIRECTUS_ADMIN_PASSWORD 
    }),
  })

  if (!response.ok) {
    throw new Error(`Erreur authentification: ${response.status}`)
  }

  const data = await response.json()
  return data.data.access_token
}

/**
 * Extrait la date de publication avec OpenAI
 */
async function extractDateWithAI(page: any, title: string, content: string): Promise<string | null> {
  if (!OPENAI_API_KEY) {
    return null
  }

  try {
    const fullHTML = await page.content()
    
    // Chercher d'abord dans les balises time et meta
    const timeElements = await page.$$eval('time[datetime], time', (elements: any[]) => {
      return elements.map(el => el.getAttribute('datetime') || el.textContent?.trim() || '').filter(Boolean)
    }).catch(() => [])
    
    const metaDates = await page.$$eval('meta[property="article:published_time"], meta[name="date"], meta[name="publishdate"]', (elements: any[]) => {
      return elements.map(el => el.getAttribute('content') || '').filter(Boolean)
    }).catch(() => [])

    const openai = new OpenAI({ apiKey: OPENAI_API_KEY })
    
    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        {
          role: 'system',
          content: 'Tu es un expert en extraction de dates depuis des articles web. Extrait la date de publication de l\'article. IMPORTANT: Cherche dans le CONTENU de l\'article (comme "avril 2023", "janvier 2024", etc.) et dans les m√©tadonn√©es. Retourne UNIQUEMENT la date au format ISO (YYYY-MM-DD), ou null si aucune date n\'est trouv√©e. Si tu trouves "avril 2023", retourne "2023-04-01". Si tu trouves "janvier 2024", retourne "2024-01-01". Utilise le premier jour du mois si seul le mois et l\'ann√©e sont donn√©s.'
        },
        {
          role: 'user',
          content: `Titre: ${title}\n\nContenu de l'article: ${content.substring(0, 3000)}\n\nBalises time trouv√©es: ${timeElements.join(', ')}\nM√©tadonn√©es trouv√©es: ${metaDates.join(', ')}\n\nExtrait la date de publication au format YYYY-MM-DD en cherchant dans le CONTENU de l'article (comme "avril 2023", "janvier 2024", etc.). Si aucune date n'est trouv√©e, retourne "null".`
        }
      ],
      max_tokens: 50
    })

    const extractedDate = response.choices[0]?.message?.content?.trim() || ''
    
    if (extractedDate && extractedDate !== 'null' && extractedDate.match(/^\d{4}-\d{2}-\d{2}$/)) {
      return extractedDate
    }
  } catch (error: any) {
    // Erreur silencieuse, on utilisera une date par d√©faut
  }

  return null
}

/**
 * Scrape et cr√©e les articles au fur et √† mesure
 */
async function scrapeAndCreateArticles(
  browser: any,
  token: string,
  existingArticles: Map<string, string>
): Promise<void> {
  let globalOrder = 1
  let totalCreated = 0
  
  // Recharger la liste des articles existants au d√©but (au cas o√π elle aurait chang√©)
  const client = createDirectus<Schema>(DIRECTUS_URL)
    .with(rest())
    .with(staticToken(token))
  const currentExistingArticles = await getExistingArticles(client, token)
  
  console.log(`R√©cup√©ration des articles depuis les pages ${FIRST_PAGE} √† ${LAST_PAGE}...`)
  
  for (let pageNum = FIRST_PAGE; pageNum <= LAST_PAGE; pageNum++) {
    const pageUrl = `${BASE_PAGE_URL}/${pageNum}`
    console.log(`\nüìÑ Page ${pageNum}/${LAST_PAGE}...`)
    
    const page = await browser.newPage()
    
    try {
      await page.goto(pageUrl, { 
        waitUntil: 'networkidle2',
        timeout: 30000 
      })
      
      // Attendre que le conteneur des articles soit charg√©
      await page.waitForSelector('#pro-gallery-margin-container-pro-blog', { timeout: 10000 })
      
      // Scroller pour charger tous les articles (lazy loading possible)
      await page.evaluate(() => {
        return new Promise<void>((resolve) => {
          let totalHeight = 0
          const distance = 100
          const timer = setInterval(() => {
            const scrollHeight = document.body.scrollHeight
            window.scrollBy(0, distance)
            totalHeight += distance

            if (totalHeight >= scrollHeight) {
              clearInterval(timer)
              resolve()
            }
          }, 100)
        })
      })
      
      // Attendre un peu pour que le JavaScript charge tous les articles apr√®s le scroll
      await new Promise(resolve => setTimeout(resolve, 3000))
      
      // Utiliser data-hook="item-link-wrapper" pour trouver les articles
      let articleContainers: any[] = []
      
      // S√©lecteur principal avec data-hook
      articleContainers = await page.$$('#pro-gallery-margin-container-pro-blog [data-hook="item-link-wrapper"]')
      
      console.log(`  Articles trouv√©s avec data-hook: ${articleContainers.length}`)
      
      // Si on ne trouve pas 5 articles, essayer de scroller encore plus
      if (articleContainers.length < ARTICLES_PER_PAGE) {
        // Scroller jusqu'en bas plusieurs fois
        for (let scroll = 0; scroll < 5; scroll++) {
          await page.evaluate(() => window.scrollTo(0, document.body.scrollHeight))
          await new Promise(resolve => setTimeout(resolve, 1000))
          
          articleContainers = await page.$$('#pro-gallery-margin-container-pro-blog [data-hook="item-link-wrapper"]')
          if (articleContainers.length === ARTICLES_PER_PAGE) {
            console.log(`  ‚úì Tous les articles charg√©s apr√®s ${scroll + 1} scroll(s)`)
            break
          }
        }
      }
      
      // Si on ne trouve toujours pas avec data-hook, essayer les autres s√©lecteurs en fallback
      if (articleContainers.length !== ARTICLES_PER_PAGE) {
        console.warn(`  ‚ö† Avec data-hook: ${articleContainers.length} articles trouv√©s`)
        
        const fallbackSelectors = [
          '#pro-gallery-margin-container-pro-blog .gallery-item-container.item-container-regular',
          '#pro-gallery-margin-container-pro-blog .gallery-item-container',
          '#pro-gallery-margin-container-pro-blog .item-container-regular',
        ]
        
        for (const selector of fallbackSelectors) {
          const candidates = await page.$$(selector)
          if (candidates.length === ARTICLES_PER_PAGE) {
            articleContainers = candidates
            console.log(`  ‚úì S√©lecteur fallback trouv√©: ${selector}`)
            break
          } else if (candidates.length > articleContainers.length) {
            articleContainers = candidates
          }
        }
      }
      
      const articleCount = articleContainers.length
      console.log(`  Trouv√© ${articleCount} article(s) sur la page ${pageNum}`)
      
      // V√©rifier qu'il y a le bon nombre d'articles par page
      // Derni√®re page peut avoir moins d'articles (3 au lieu de 5)
      const expectedCount = pageNum === LAST_PAGE ? 3 : ARTICLES_PER_PAGE
      
      if (articleCount !== expectedCount) {
        console.warn(`  ‚ö† ATTENTION: ${articleCount} articles trouv√©s au lieu de ${expectedCount} sur la page ${pageNum}`)
        // Si on n'a pas le bon nombre, arr√™ter pour cette page
        if (articleCount === 0) {
          console.warn(`  ‚ö† Aucun article trouv√©, passage √† la page suivante`)
          continue
        }
        // Si on a moins d'articles que pr√©vu mais pas z√©ro, continuer quand m√™me
      } else {
        console.log(`  ‚úì Correct: ${articleCount} articles trouv√©s`)
      }
      
      // Pour chaque article, cliquer dessus et scraper le contenu
      for (let i = 0; i < articleContainers.length; i++) {
        try {
          // Re-r√©cup√©rer les conteneurs √† chaque it√©ration car la page peut changer
          articleContainers = await page.$$('#pro-gallery-margin-container-pro-blog [data-hook="item-link-wrapper"]')
          
          if (i >= articleContainers.length) {
            console.warn(`    Index ${i} hors limites, arr√™t du scraping de cette page`)
            break
          }
          
          console.log(`  Article ${i + 1}/${articleContainers.length}...`)
          
          // Trouver le lien dans le conteneur et cliquer dessus
          const currentUrl = page.url()
          
          // R√©cup√©rer l'URL du lien depuis l'√©l√©ment
          const articleUrl = await articleContainers[i].evaluate((el: any) => {
            const link = el.querySelector('a')
            if (link && link.href) {
              return link.href
            }
            // Si pas de lien direct, chercher dans les attributs
            return el.getAttribute('href') || el.getAttribute('data-href') || null
          })
          
          if (articleUrl) {
            // Aller directement √† l'URL de l'article
            console.log(`    Navigation vers: ${articleUrl}`)
            await page.goto(articleUrl, { waitUntil: 'networkidle2', timeout: 30000 })
          } else {
            // Si pas d'URL, cliquer sur l'√©l√©ment
            await articleContainers[i].click()
            // Attendre la navigation
            await page.waitForNavigation({ waitUntil: 'networkidle2', timeout: 15000 })
          }
          
          // Attendre que le contenu soit charg√©
          await page.waitForSelector('#content-wrapper', { timeout: 15000 })
          // Attendre un peu plus pour que tout le contenu se charge
          await new Promise(resolve => setTimeout(resolve, 2000))
          
          // Scraper l'article depuis la page actuelle
          const article = await scrapeArticleFromPage(page, globalOrder)
          
          if (article) {
            // Cr√©er l'article imm√©diatement apr√®s le scraping
            await processAndCreateArticle(article, globalOrder, token, currentExistingArticles)
            // Mettre √† jour la liste des articles existants apr√®s cr√©ation
            const updatedExisting = await getExistingArticles(client, token)
            currentExistingArticles.clear()
            updatedExisting.forEach((value, key) => currentExistingArticles.set(key, value))
            totalCreated++
            globalOrder++
          }
          
          // Revenir √† la page de liste
          // Utiliser l'URL de la page plut√¥t que goBack pour √™tre s√ªr
          await page.goto(pageUrl, { waitUntil: 'networkidle2' })
          await page.waitForSelector('#pro-gallery-margin-container-pro-blog', { timeout: 10000 })
          
          // Scroller √† nouveau pour recharger les articles
          await page.evaluate(() => {
            return new Promise<void>((resolve) => {
              let totalHeight = 0
              const distance = 100
              const timer = setInterval(() => {
                const scrollHeight = document.body.scrollHeight
                window.scrollBy(0, distance)
                totalHeight += distance

                if (totalHeight >= scrollHeight) {
                  clearInterval(timer)
                  resolve()
                }
              }, 100)
            })
          })
          
          await new Promise(resolve => setTimeout(resolve, 2000)) // Pause pour laisser la page se recharger
          
        } catch (error: any) {
          console.warn(`    ‚ö† Erreur pour l'article ${i + 1} de la page ${pageNum}:`, error.message)
          // Essayer de revenir √† la page de liste en cas d'erreur
          try {
            await page.goto(pageUrl, { waitUntil: 'networkidle2' })
            await page.waitForSelector('#pro-gallery-margin-container-pro-blog', { timeout: 10000 })
          } catch (e) {
            // Si on ne peut pas revenir, passer √† la page suivante
            break
          }
        }
      }
      
    } catch (error: any) {
      console.warn(`  ‚ö† Erreur sur la page ${pageNum}:`, error.message)
    } finally {
      await page.close()
    }
    
    // Pause entre les pages
    await new Promise(resolve => setTimeout(resolve, 2000))
  }
  
  console.log(`\n‚úÖ Migration termin√©e! ${totalCreated} article(s) cr√©√©(s) au total`)
}

/**
 * Scrape un article depuis la page actuelle (d√©j√† sur la page de l'article)
 */
async function scrapeArticleFromPage(page: any, order: number): Promise<ScrapedArticle | null> {
  try {
    const currentUrl = page.url()
    
    // Attendre que le contenu soit compl√®tement charg√©
    await page.waitForSelector('#content-wrapper section', { timeout: 10000 })
    // Attendre un peu plus pour le chargement asynchrone
    await new Promise(resolve => setTimeout(resolve, 2000))
    
    // Extraire le titre depuis #content-wrapper header
    let title = ''
    try {
      // Attendre que le header soit pr√©sent
      await page.waitForSelector('#content-wrapper header', { timeout: 8000 })
      title = await page.$eval('#content-wrapper header', (el: any) => {
        // Chercher le h1 ou le premier √©l√©ment de texte significatif
        const h1 = el.querySelector('h1')
        if (h1) return h1.textContent?.trim() || ''
        return el.textContent?.trim() || ''
      })
    } catch (error) {
      // Essayer d'autres s√©lecteurs pour le titre
      try {
        await page.waitForSelector('h1', { timeout: 5000 })
        title = await page.$eval('h1', (el: any) => el.textContent?.trim() || '')
      } catch (e) {
        title = await page.title().catch(() => '')
        // Nettoyer le titre de la page
        title = title.replace(/\s*[-|]\s*.*$/, '')
      }
    }
    
    if (!title || title.length < 3) {
      console.warn(`    Pas de titre valide trouv√©`)
      return null
    }
    
    // Extraire le sous-titre (peut √™tre dans le header aussi)
    let subtitle: string | undefined = undefined
    try {
      subtitle = await page.$eval('#content-wrapper header', (el: any) => {
        const subtitleEl = el.querySelector('h2, .subtitle, [class*="subtitle"]')
        return subtitleEl?.textContent?.trim() || ''
      })
    } catch (error) {
      // Essayer d'autres s√©lecteurs
      try {
        subtitle = await page.$eval('h2', (el: any) => el.textContent?.trim() || '').catch(() => undefined)
      } catch (e) {
        subtitle = undefined
      }
    }
    
    // Contenu - extraire le texte proprement depuis la section (comme dans migrate-articles-simple.ts)
    const fullHTML = await page.content()
    const $ = require('cheerio').load(fullHTML)
    
    // Extraire la section
    const section = $('#content-wrapper section')
    let contentText = ''
    let sectionHTML = '' // Garder le HTML pour extraire les images
    
    if (section.length > 0) {
      // Enlever les scripts, styles, etc.
      section.find('script, style, nav, .menu, .navigation, .sidebar, aside').remove()
      
      // Garder le HTML pour les images
      sectionHTML = section.html() || ''
      
      // Extraire le texte en pr√©servant les paragraphes et en convertissant les strong/b en ##
      const paragraphs: string[] = []
      
      // Traiter les paragraphes un par un
      section.find('p').each((_: any, p: any) => {
        const $p = $(p)
        const $clone = $p.clone()
        
        // Convertir les strong/b en ## avec saut de ligne apr√®s
        $clone.find('strong, b').each((_: any, el: any) => {
          const $el = $(el)
          const text = $el.text().trim()
          if (text && text.length > 0 && text.length < 100) {
            // Remplacer le strong/b par un h2 markdown suivi d'un saut de ligne
            $el.replaceWith(`## ${text}\n\n`)
          }
        })
        
        // Nettoyer le HTML restant et extraire le texte
        let text = $clone.html() || ''
        
        // Nettoyer les balises HTML restantes mais garder la structure
        text = text
          .replace(/<br\s*\/?>/gi, '\n')
          .replace(/<\/p>/gi, '\n\n')
          .replace(/<[^>]+>/g, '') // Enlever toutes les balises HTML restantes
          .trim()
        
        if (text && text.length > 10) {
          paragraphs.push(text)
        }
      })
      
      // Si pas de paragraphes, prendre tout le texte
      if (paragraphs.length === 0) {
        contentText = section.text().trim()
        // Diviser en paragraphes approximatifs (par double saut de ligne)
        paragraphs.push(...contentText.split(/\n\s*\n/).filter(p => p.trim().length > 10))
      } else {
        contentText = paragraphs.join('\n\n')
      }
    } else {
      // Si pas de section, prendre le content-wrapper sans le header
      const contentWrapper = $('#content-wrapper')
      if (contentWrapper.length > 0) {
        contentWrapper.find('header, script, style, nav, .menu, .navigation').remove()
        
        // Traiter les paragraphes
        const paragraphs: string[] = []
        contentWrapper.find('p').each((_: any, p: any) => {
          const $p = $(p)
          const $clone = $p.clone()
          
          // Convertir les strong/b en ##
          $clone.find('strong, b').each((_: any, el: any) => {
            const $el = $(el)
            const text = $el.text().trim()
            if (text && text.length > 0 && text.length < 100) {
              $el.replaceWith(`## ${text}\n\n`)
            }
          })
          
          let text = $clone.html() || ''
          text = text
            .replace(/<br\s*\/?>/gi, '\n')
            .replace(/<\/p>/gi, '\n\n')
            .replace(/<[^>]+>/g, '')
            .trim()
          
          if (text && text.length > 10) {
            paragraphs.push(text)
          }
        })
        
        sectionHTML = contentWrapper.html() || ''
        contentText = paragraphs.length > 0 ? paragraphs.join('\n\n') : contentWrapper.text().trim()
      }
    }
    
    // Nettoyer le texte et s'assurer que les ## sont bien format√©s
    contentText = contentText
      .replace(/\*\*(.*?)\*\*/g, '## $1\n\n') // Markdown bold vers h2
      .replace(/\*(.*?)\*/g, '$1') // Enlever les italiques simples
      .replace(/\n{3,}/g, '\n\n') // Remplacer les sauts de ligne multiples (mais garder les doubles)
      .replace(/##\s+##/g, '##') // Nettoyer les doubles ##
      .replace(/##\s+([^\n]+)\n\n([^\n#])/g, '## $1\n\n$2') // S'assurer qu'il y a un saut de ligne apr√®s ##
      .trim()
    
    // Si le contenu est vide ou trop court, utiliser l'IA
    let markdown = contentText
    if (!markdown || markdown.length < 100) {
      if (OPENAI_API_KEY) {
        console.log(`    ‚ö† Contenu trop court, utilisation de l'IA pour extraire...`)
        const openai = new OpenAI({ apiKey: OPENAI_API_KEY })
        
        try {
          const response = await openai.chat.completions.create({
            model: 'gpt-4',
            messages: [
              {
                role: 'system',
                content: 'Tu es un expert en extraction de contenu web. Extrait UNIQUEMENT le contenu principal de l\'article (texte en paragraphes) depuis le HTML fourni. Retourne le contenu en texte brut ou markdown simple, SANS HTML, SANS le titre (qui est d√©j√† extrait s√©par√©ment). Structure le contenu en paragraphes clairs s√©par√©s par des sauts de ligne. Ne garde que le texte principal, pas les menus, footers, etc.'
              },
              {
                role: 'user',
                content: `Titre de l'article: ${title}\n\nExtrait le contenu principal de l'article depuis ce HTML. Cherche dans #content-wrapper section. Retourne uniquement le texte en paragraphes, sans HTML:\n\n${fullHTML.substring(0, 100000)}`
              }
            ],
            max_tokens: 4000
          })

          const extractedContent = response.choices[0]?.message?.content || ''
          if (extractedContent && extractedContent.length > 50) {
            let cleaned = extractedContent
              .replace(/```markdown\n?/g, '')
              .replace(/```\n?/g, '')
              .replace(/<[^>]+>/g, '') // Enlever tout HTML restant
              .trim()
            
            if (cleaned.length > 50) {
              markdown = cleaned
              console.log(`    ‚úì Contenu extrait via IA (${cleaned.length} caract√®res)`)
            }
          }
        } catch (error: any) {
          console.warn(`    ‚ö† Erreur avec OpenAI:`, error.message)
        }
      }
    }
    
    // Structurer le contenu en paragraphes markdown
    if (markdown && markdown.length > 0) {
      // Diviser en paragraphes si ce n'est pas d√©j√† fait
      const paragraphs = markdown.split(/\n\s*\n/).filter(p => p.trim().length > 0)
      if (paragraphs.length > 1) {
        markdown = paragraphs.map(p => p.trim()).join('\n\n')
      } else {
        // Si un seul bloc, essayer de le diviser intelligemment
        const sentences = markdown.split(/[.!?]\s+/).filter(s => s.trim().length > 20)
        if (sentences.length > 3) {
          // Grouper les phrases en paragraphes de 2-3 phrases
          const grouped: string[] = []
          for (let i = 0; i < sentences.length; i += 2) {
            grouped.push(sentences.slice(i, i + 2).join('. '))
          }
          markdown = grouped.join('\n\n')
        }
      }
      
      // Nettoyer les titres h2 en double
      markdown = markdown.replace(/\n##\s+/g, '\n## ').replace(/##\s+##/g, '##')
    }
    
    if (!markdown || markdown.length < 50) {
      console.warn(`    Markdown trop court (${markdown.length} caract√®res)`)
      return null
    }
    
    console.log(`    ‚úì Markdown g√©n√©r√© (${markdown.length} caract√®res)`)
    
    // Extraire l'image depuis [data-hook="image-viewer-au4eo"]
    const images: string[] = []
    
    try {
      // Attendre que l'image soit charg√©e
      await page.waitForSelector('[data-hook="image-viewer-au4eo"]', { timeout: 5000 }).catch(() => {})
      
      const imageElement = await page.$('[data-hook="image-viewer-au4eo"]')
      if (imageElement) {
        const imageSrc = await imageElement.evaluate((el: any) => {
          // Essayer plusieurs attributs
          return el.src || 
                 el.getAttribute('src') || 
                 el.getAttribute('data-src') || 
                 el.getAttribute('data-lazy-src') ||
                 el.getAttribute('data-original') ||
                 (el.style && el.style.backgroundImage ? el.style.backgroundImage.replace(/url\(['"]?(.*?)['"]?\)/, '$1') : '')
        })
        
        if (imageSrc && imageSrc.trim()) {
          let imageUrl = imageSrc.trim()
          // Nettoyer l'URL si c'est un background-image
          imageUrl = imageUrl.replace(/^url\(['"]?/, '').replace(/['"]?\)$/, '')
          
          if (!imageUrl.startsWith('http')) {
            imageUrl = `${SOURCE_URL}${imageUrl.startsWith('/') ? '' : '/'}${imageUrl}`
          }
          
          if (imageUrl && !imageUrl.includes('data:image')) {
            images.push(imageUrl)
          }
        }
      }
    } catch (error: any) {
      // Pas d'image principale, ce n'est pas grave
    }
    
    // Images dans le contenu HTML - utiliser le HTML de la section d√©j√† extrait
    if (sectionHTML) {
      const $content = require('cheerio').load(sectionHTML)
      $content('img').each((_: any, img: any) => {
        const src = $content(img).attr('src') || 
                   $content(img).attr('data-src') || 
                   $content(img).attr('data-lazy-src') ||
                   $content(img).attr('data-original')
        if (src && !src.includes('data:image') && !src.includes('placeholder')) {
          let imageUrl = src
          if (!imageUrl.startsWith('http')) {
            imageUrl = `${SOURCE_URL}${imageUrl.startsWith('/') ? '' : '/'}${imageUrl}`
          }
          if (!images.includes(imageUrl)) {
            images.push(imageUrl)
          }
        }
      })
    }
    
    // Chercher aussi dans tout le body de la page
    try {
      const allImages = await page.$$eval('img[data-hook="image-viewer-au4eo"], [data-hook="image-viewer-au4eo"] img, #content-wrapper img', (imgs: any[]) => {
        return imgs.map((img: any) => {
          return img.src || img.getAttribute('src') || img.getAttribute('data-src') || ''
        }).filter((src: string) => src && !src.includes('data:image'))
      })
      
      allImages.forEach((src: string) => {
        let imageUrl = src
        if (!imageUrl.startsWith('http')) {
          imageUrl = `${SOURCE_URL}${imageUrl.startsWith('/') ? '' : '/'}${imageUrl}`
        }
        if (imageUrl && !images.includes(imageUrl)) {
          images.push(imageUrl)
        }
      })
    } catch (error) {
      // Ignorer les erreurs
    }
    
    // Extraire la date avec l'IA
    let date = await extractDateWithAI(page, title, markdown)
    
    // Si pas de date trouv√©e, utiliser la date actuelle
    if (!date) {
      date = new Date().toISOString().split('T')[0]
    }
    
    // G√©n√©rer un slug depuis l'URL ou le titre
    const urlParts = currentUrl.split('/').filter(Boolean)
    let slug = urlParts[urlParts.length - 1]?.replace(/\.html?$/, '') || ''
    
    // D√©coder l'URL si elle est encod√©e
    try {
      slug = decodeURIComponent(slug)
    } catch {
      // Si le d√©codage √©choue, garder le slug tel quel
    }
    
    // Si pas de slug depuis l'URL, g√©n√©rer depuis le titre
    if (!slug || slug.length < 3) {
      slug = title.toLowerCase()
        .normalize('NFD')
        .replace(/[\u0300-\u036f]/g, '')
        .replace(/[^a-z0-9]+/g, '-')
        .replace(/^-+|-+$/g, '')
    }
    
    // Nettoyer le slug pour √©viter les caract√®res probl√©matiques
    slug = slug
      .replace(/[^a-z0-9-]/g, '-')
      .replace(/-+/g, '-')
      .replace(/^-+|-+$/g, '')
      .substring(0, 100) // Limiter la longueur
    
    return {
      title,
      subtitle: subtitle && subtitle.length > 5 ? subtitle : undefined,
      content: markdown,
      images,
      date,
      slug,
      order
    }
  } catch (error: any) {
    console.error(`    Erreur lors du scraping:`, error.message)
    return null
  }
}



/**
 * R√©cup√®re tous les articles existants dans Directus
 */
async function getExistingArticles(
  client: ReturnType<typeof createDirectus<Schema>>,
  token: string
): Promise<Map<string, string>> {
  try {
    const response = await fetch(`${DIRECTUS_URL}/items/actus?fields=id,slug`, {
      method: 'GET',
      headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/json',
      },
    })
    
    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${await response.text()}`)
    }
    
    const result = await response.json()
    const articles = result.data || []
    const slugMap = new Map<string, string>() // slug -> id
    
    if (Array.isArray(articles)) {
      articles.forEach((article: any) => {
        if (article.slug) {
          slugMap.set(article.slug, article.id)
        }
      })
    }
    
    return slugMap
  } catch (error: any) {
    console.warn('‚ö† Impossible de r√©cup√©rer les articles existants:', error.message || error)
    return new Map()
  }
}

/**
 * Supprime tous les articles existants dans Directus
 */
async function deleteAllArticles(token: string): Promise<number> {
  try {
    // R√©cup√©rer tous les IDs
    const response = await fetch(`${DIRECTUS_URL}/items/actus?fields=id`, {
      method: 'GET',
      headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/json',
      },
    })
    
    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${await response.text()}`)
    }
    
    const result = await response.json()
    const articles = result.data || []
    const ids = Array.isArray(articles) ? articles.map((a: any) => a.id).filter(Boolean) : []
    
    if (ids.length === 0) {
      return 0
    }
    
    // Supprimer tous les articles
    await fetch(`${DIRECTUS_URL}/items/actus`, {
      method: 'DELETE',
      headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(ids),
    })
    
    console.log(`‚úì ${ids.length} article(s) supprim√©(s)`)
    return ids.length
  } catch (error: any) {
    console.error('‚ùå Erreur lors de la suppression des articles:', error.message)
    throw error
  }
}

/**
 * Met √† jour un article existant dans Directus
 */
async function updateArticleInDirectus(
  client: ReturnType<typeof createDirectus<Schema>>,
  articleId: string,
  article: ScrapedArticle,
  coverImageId: string | null
): Promise<void> {
  try {
    // Ajouter le titre en H1 au d√©but du markdown
    const bodyWithTitle = `# ${article.title}\n\n${article.content}`
    
    const articleData: any = {
      title: article.title,
      body: bodyWithTitle,
      date: article.date || new Date().toISOString().split('T')[0],
      slug: article.slug,
    }
    
    if (article.subtitle && article.subtitle.length > 0) {
      articleData.subtitle = article.subtitle
    }
    
    if (coverImageId) {
      articleData.cover = coverImageId
    }
    
    // Utiliser l'API REST directement
    const token = (client as any).authentication?.token || ''
    const response = await fetch(`${DIRECTUS_URL}/items/actus/${articleId}`, {
      method: 'PATCH',
      headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(articleData),
    })
    
    if (!response.ok) {
      const errorData = await response.text()
      throw new Error(`Erreur HTTP ${response.status}: ${errorData}`)
    }
    
    console.log(`‚úì Article mis √† jour: ${article.title}`)
  } catch (error: any) {
    console.error(`Erreur lors de la mise √† jour de l'article "${article.title}":`, error.message)
    throw error
  }
}

/**
 * Cr√©e un nouvel article dans Directus
 */
async function createArticleInDirectus(
  client: ReturnType<typeof createDirectus<Schema>>,
  article: ScrapedArticle,
  coverImageId: string | null,
  token?: string
): Promise<void> {
  // Ajouter le titre en H1 au d√©but du markdown
  const bodyWithTitle = `# ${article.title}\n\n${article.content}`
  
  const articleData: any = {
    title: article.title,
    body: bodyWithTitle,
    date: article.date || new Date().toISOString().split('T')[0],
    slug: article.slug,
  }
  
  if (article.subtitle && article.subtitle.length > 0) {
    articleData.subtitle = article.subtitle
  }
  
  if (coverImageId) {
    articleData.cover = coverImageId
  }
  
  // V√©rifier que le slug est valide
  if (!article.slug || article.slug.length === 0) {
    throw new Error(`Slug invalide pour l'article "${article.title}"`)
  }
  
  try {
    // Utiliser l'API REST directement pour avoir de meilleurs messages d'erreur
    if (!token) {
      throw new Error('Token manquant pour cr√©er l\'article')
    }
    const authToken = token
    
    const response = await fetch(`${DIRECTUS_URL}/items/actus`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${authToken}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(articleData),
    })
    
    if (!response.ok) {
      const errorData = await response.text()
      let errorJson
      try {
        errorJson = JSON.parse(errorData)
      } catch {
        errorJson = errorData
      }
      throw new Error(`Erreur HTTP ${response.status}: ${JSON.stringify(errorJson)}`)
    }
    
    const result = await response.json()
    console.log(`‚úì Article cr√©√©: ${article.title}`)
    return result
  } catch (error: any) {
    const errorMessage = error.message || error.toString() || 'Erreur inconnue'
    console.error(`Erreur lors de la cr√©ation de l'article "${article.title}":`, errorMessage)
    
    // Afficher plus de d√©tails sur l'erreur
    if (error.response) {
      console.error(`  Status: ${error.response.status}`)
      console.error(`  D√©tails:`, JSON.stringify(error.response.data || error.response, null, 2))
    } else if (error.errors) {
      console.error(`  Erreurs:`, JSON.stringify(error.errors, null, 2))
    } else if (error.data) {
      console.error(`  Donn√©es d'erreur:`, JSON.stringify(error.data, null, 2))
    }
    
    // Afficher les donn√©es qui ont √©t√© envoy√©es
    console.error(`  Donn√©es envoy√©es:`, JSON.stringify(articleData, null, 2))
    
    throw error
  }
}

/**
 * Traite un article (t√©l√©charge les images, upload, cr√©e dans Directus)
 */
async function processAndCreateArticle(
  article: ScrapedArticle,
  order: number,
  token: string,
  existingArticles: Map<string, string>
): Promise<void> {
  console.log(`\nüìÑ Article ${order}: ${article.title}`)
  
  let coverImageId: string | null = null
  const imageMap = new Map<string, string>() // Map URL -> Directus ID
  
  // T√©l√©charger et uploader toutes les images
  if (article.images.length > 0) {
    console.log(`  üì∏ ${article.images.length} image(s) √† traiter...`)
    
    for (let i = 0; i < article.images.length; i++) {
      const imageUrl = article.images[i]
      const urlObj = new URL(imageUrl)
      
      // Utiliser le slug de l'article pour nommer l'image
      const slug = article.slug || article.title.toLowerCase()
        .normalize('NFD')
        .replace(/[\u0300-\u036f]/g, '')
        .replace(/[^a-z0-9]+/g, '-')
        .replace(/^-+|-+$/g, '')
      
      // D√©terminer l'extension du fichier original
      const originalFilename = path.basename(urlObj.pathname)
      const extension = path.extname(originalFilename) || '.jpg'
      
      // Cr√©er un nom de fichier bas√© sur le slug de l'article
      const imageFilename = i === 0 ? `${slug}${extension}` : `${slug}-${i + 1}${extension}`
      const tempImagePath = path.join(TEMP_DIR, imageFilename)
      
      try {
        if (!DRY_RUN) {
          console.log(`    T√©l√©chargement: ${imageFilename}`)
          await downloadFile(imageUrl, tempImagePath)
          
          const uploadedId = await uploadImageToDirectus(tempImagePath, imageFilename, token)
          
          if (uploadedId) {
            imageMap.set(imageUrl, uploadedId)
            if (i === 0) {
              coverImageId = uploadedId
            }
            console.log(`    ‚úì Image upload√©e (ID: ${uploadedId})`)
          }
          
          // Nettoyer le fichier temporaire
          if (fs.existsSync(tempImagePath)) {
            fs.unlinkSync(tempImagePath)
          }
        } else {
          console.log(`    [DRY-RUN] Image serait t√©l√©charg√©e: ${imageFilename}`)
        }
      } catch (error: any) {
        console.warn(`    ‚ö† Erreur image ${imageFilename}:`, error.message)
      }
    }
  }
  
  // Cr√©er l'article dans Directus
  if (!DRY_RUN) {
    try {
      const client = createDirectus<Schema>(DIRECTUS_URL)
        .with(rest())
        .with(staticToken(token))
      
      // V√©rifier si l'article existe d√©j√†
      const existingId = existingArticles.get(article.slug || '')
      
      if (existingId && UPDATE_EXISTING) {
        console.log(`  üîÑ Mise √† jour de l'article existant...`)
        await updateArticleInDirectus(client, existingId, article, coverImageId)
      } else if (!existingId) {
        console.log(`  ‚ûï Cr√©ation de l'article dans Directus...`)
        console.log(`     Slug: ${article.slug}`)
        console.log(`     Titre: ${article.title}`)
        console.log(`     Date: ${article.date}`)
        await createArticleInDirectus(client, article, coverImageId, token)
        console.log(`  ‚úÖ Article cr√©√© avec succ√®s!`)
      } else {
        console.log(`  ‚ö† Article d√©j√† existant (slug: ${article.slug}), ignor√©`)
      }
    } catch (error: any) {
      const errorMessage = error.message || error.toString() || 'Erreur inconnue'
      console.error(`  ‚ùå Erreur lors de la cr√©ation:`, errorMessage)
      
      // Afficher plus de d√©tails
      if (error.response) {
        console.error(`    Status: ${error.response.status}`)
        console.error(`    D√©tails:`, JSON.stringify(error.response.data || error.response, null, 2))
      } else if (error.errors) {
        console.error(`    Erreurs:`, JSON.stringify(error.errors, null, 2))
      } else if (error.data) {
        console.error(`    Donn√©es d'erreur:`, JSON.stringify(error.data, null, 2))
      } else {
        console.error(`    Erreur compl√®te:`, JSON.stringify(error, Object.getOwnPropertyNames(error), 2))
      }
      
      // Afficher les donn√©es de l'article pour debug
      console.error(`    Donn√©es de l'article:`, JSON.stringify({
        title: article.title,
        slug: article.slug,
        date: article.date,
        hasSubtitle: !!article.subtitle,
        hasCover: !!coverImageId,
        contentLength: article.content.length
      }, null, 2))
      
      throw error
    }
  } else {
    console.log(`  [DRY-RUN] Article serait cr√©√©: ${article.title}`)
  }
}

/**
 * Fonction principale
 */
async function main() {
  console.log('üöÄ D√©marrage de la migration des articles...\n')
  
  // 1. Authentification Directus
  console.log('1. Authentification Directus...')
  let client: ReturnType<typeof createDirectus<Schema>>
  let token: string
  
  if (DIRECTUS_STATIC_TOKEN) {
    token = DIRECTUS_STATIC_TOKEN
    client = createDirectus<Schema>(DIRECTUS_URL)
      .with(rest())
      .with(staticToken(DIRECTUS_STATIC_TOKEN))
  } else {
    token = await getAdminToken()
    client = createDirectus<Schema>(DIRECTUS_URL)
      .with(rest())
      .with(staticToken(token))
  }
  console.log('‚úì Authentifi√©\n')
  
  // 1.5. V√©rifier et g√©rer les articles existants
  console.log('1.5. V√©rification des articles existants...')
  const existingArticles = await getExistingArticles(client, token)
  console.log(`‚úì ${existingArticles.size} article(s) existant(s) trouv√©(s)`)
  
  if (existingArticles.size > 0) {
    if (FORCE && !DRY_RUN) {
      console.log(`\nüóëÔ∏è  Suppression de ${existingArticles.size} article(s) existant(s)...`)
      const deletedCount = await deleteAllArticles(token)
      console.log(`‚úì ${deletedCount} article(s) supprim√©(s)\n`)
    } else if (FORCE && DRY_RUN) {
      console.log(`\n[DRY-RUN] ${existingArticles.size} article(s) seraient supprim√©(s)\n`)
    } else if (!UPDATE_EXISTING) {
      console.warn(`\n‚ö†Ô∏è  ATTENTION: ${existingArticles.size} article(s) existant(s) dans Directus!`)
      console.warn(`   Utilisez --force pour les supprimer avant la migration`)
      console.warn(`   Ou utilisez --update pour mettre √† jour les articles existants au lieu de cr√©er des doublons`)
      console.warn(`   Sinon, des doublons seront cr√©√©s!\n`)
    } else {
      console.log(`\n‚úì Mode UPDATE: les articles existants seront mis √† jour\n`)
    }
  } else {
    console.log('‚úì Aucun article existant\n')
  }
  
  // 2. Lancer Puppeteer
  console.log('2. Lancement du navigateur...')
  const browser = await puppeteer.launch({
    headless: true,
    args: ['--no-sandbox', '--disable-setuid-sandbox']
  })
  console.log('‚úì Navigateur lanc√©\n')
  
  try {
    // 3. Scraper et cr√©er les articles au fur et √† mesure
    console.log('3. Scraping et cr√©ation des articles au fur et √† mesure...\n')
    if (DRY_RUN) {
      console.log('‚ö† Mode DRY-RUN activ√© : aucun article ne sera cr√©√©\n')
    }
    
    const token = DIRECTUS_STATIC_TOKEN || await getAdminToken()
    
    // Scraper et cr√©er les articles page par page
    await scrapeAndCreateArticles(browser, token, existingArticles)
    
    // 6. Nettoyer le dossier temporaire
    console.log('\n6. Nettoyage...')
    if (fs.existsSync(TEMP_DIR)) {
      fs.rmSync(TEMP_DIR, { recursive: true, force: true })
    }
    
    console.log('\n‚úÖ Migration termin√©e!')
  } finally {
    // Fermer le navigateur
    await browser.close()
    console.log('‚úì Navigateur ferm√©')
  }
}

// Ex√©cuter le script
main().catch((error) => {
  console.error('‚ùå Erreur fatale:', error)
  process.exit(1)
})

